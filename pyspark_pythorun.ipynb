{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZnuT9XRHCiyDPE4j3tIrd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marreapato/Deep_Learning_Course/blob/main/pyspark_pythorun.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ2yHaWVhXDT",
        "outputId": "4d625950-741e-402c-ba2d-cb4da6744f4a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/mlcrs/\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgmm6PxbhXZy",
        "outputId": "05a74b4f-fe5c-40ce-f2a0-92306bff33f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/mlcrs\n",
            "access_log.txt\t\t\t      KFoldCrossValidation.ipynb     Percentiles.ipynb\n",
            "breakfast.jpg\t\t\t      KMeans.ipynb\t\t     PoliticsExercise.ipynb\n",
            "bridge.jpg\t\t\t      LinearRegression.ipynb\t     PolynomialRegression.ipynb\n",
            "bunny.jpg\t\t\t      mammographic_masses.data.txt   Python101.ipynb\n",
            "castle.jpg\t\t\t      mammographic_masses.names.txt  regression.txt\n",
            "ConditionalProbabilityExercise.ipynb  mammo_masses_project.ipynb     Seaborn.ipynb\n",
            "ConditionalProbabilitySolution.ipynb  MatPlotLib.ipynb\t\t     SparkDecisionTree.py\n",
            "CovarianceCorrelation.ipynb\t      MeanMedianExercise.ipynb\t     SparkKMeans.py\n",
            "DecisionTree.ipynb\t\t      MeanMedianMode.ipynb\t     SparkLinearRegression.py\n",
            "DeepLearningProject.ipynb\t      ml-100k\t\t\t     SparkPCA.py\n",
            "DeepLearningProject-Solution.ipynb    MLCourse.zip\t\t     StdDevVariance.ipynb\n",
            "distancerecomendation-KNN.ipynb       mlcrsItemBasedCF.ipynb\t     subset-small.tsv\n",
            "Distributions.ipynb\t\t      mlcrsQ-Learning.ipynb\t     SVC.ipynb\n",
            "emails\t\t\t\t      mlcrs_SimilarMovies.ipynb      Tensorflow.ipynb\n",
            "fighterjet.jpg\t\t\t      MNIST_data\t\t     TF-IDF.py\n",
            "FinalProjectAssignment.ipynb\t      Moments.ipynb\t\t     TopPages.ipynb\n",
            "firetruck.jpg\t\t\t      MultipleRegression.ipynb\t     TrainTest.ipynb\n",
            "GAN_on_Fashion_MNIST.ipynb\t      MyPlot.png\t\t     TransferLearning.ipynb\n",
            "GenAI\t\t\t\t      NaiveBayes.ipynb\t\t     TTest.ipynb\n",
            "house-votes-84.data.txt\t\t      Outliers.ipynb\t\t     VariationalAutoEncoders.ipynb\n",
            "house-votes-84.names.txt\t      PandasTutorial.ipynb\t     VLA.jpg\n",
            "Keras-CNN.ipynb\t\t\t      PastHires.csv\t\t     XGBoost.ipynb\n",
            "Keras.ipynb\t\t\t      PCA.ipynb\n",
            "Keras-RNN.ipynb\t\t\t      peek.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/gdrive/MyDrive/mlcrs/PastHires.csv\""
      ],
      "metadata": {
        "id": "CVprrAAZhZUT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "qAfn8SU4aV4p",
        "outputId": "693f9fee-1d68-4a4d-b244-f61d012e16e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [1 InRelease 0 B/3,626 B 0%] [Co\u001b[0m\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcont\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,263 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,309 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,635 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [50.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,621 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,340 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,049 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,161 kB]\n",
            "Fetched 10.7 MB in 4s (2,526 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "26 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=d8f569e3cc7076af295830f16b5d227baf78cdc75caef7da8ddcc933da143498\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7aac08775690>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0c2726da587b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "T415r7Q6cK33",
        "outputId": "6d49e4dd-1b6e-4f23-cac9-e47f1f8f2729"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7aac08775690>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0c2726da587b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1D6OnENTZVb1"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.regression import LabeledPoint\n",
        "from pyspark.mllib.tree import DecisionTree\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from numpy import array\n",
        "\n",
        "# Boilerplate Spark stuff:\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"SparkDecisionTree\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some functions that convert our CSV input data into numerical\n",
        "# features for each job candidate\n",
        "def binary(YN):\n",
        "    if (YN == 'Y'):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def mapEducation(degree):\n",
        "    if (degree == 'BS'):\n",
        "        return 1\n",
        "    elif (degree =='MS'):\n",
        "        return 2\n",
        "    elif (degree == 'PhD'):\n",
        "        return 3\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Convert a list of raw fields from our CSV file to a\n",
        "# LabeledPoint that MLLib can use. All data must be numerical...\n",
        "def createLabeledPoints(fields):\n",
        "    yearsExperience = int(fields[0])\n",
        "    employed = binary(fields[1])\n",
        "    previousEmployers = int(fields[2])\n",
        "    educationLevel = mapEducation(fields[3])\n",
        "    topTier = binary(fields[4])\n",
        "    interned = binary(fields[5])\n",
        "    hired = binary(fields[6])\n",
        "\n",
        "    return LabeledPoint(hired, array([yearsExperience, employed,\n",
        "        previousEmployers, educationLevel, topTier, interned]))\n",
        "\n",
        "#Load up our CSV file, and filter out the header line with the column names\n",
        "rawData = sc.textFile(data_dir)\n",
        "header = rawData.first()\n"
      ],
      "metadata": {
        "id": "k_sZvbc1abPX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rawData"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LwavTZ6fmwa",
        "outputId": "c9ef6e7a-21d6-4fda-9673-a80d3f2e5f1e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/gdrive/MyDrive/mlcrs/PastHires.csv MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "header"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qhmOlTfJfkhs",
        "outputId": "1f42d61c-fdbf-4cb5-98eb-1a4d741b3023"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Years Experience,Employed?,Previous employers,Level of Education,Top-tier school,Interned,Hired'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rawData = rawData.filter(lambda x:x != header)\n",
        "\n",
        "# Split each line into a list based on the comma delimiters\n",
        "csvData = rawData.map(lambda x: x.split(\",\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "ATPS8sEvciqr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csvData"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIAXgjuof1Xv",
        "outputId": "dac840f9-4b20-4889-b2b3-ce924d33b2c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[5] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert these lists to LabeledPoints\n",
        "trainingData = csvData.map(createLabeledPoints)\n",
        "\n",
        "# Create a test candidate, with 10 years of experience, currently employed,\n",
        "# 3 previous employers, a BS degree, but from a non-top-tier school where\n",
        "# he or she did not do an internship. You could of course load up a whole\n",
        "# huge RDD of test candidates from disk, too.\n",
        "testCandidates = [ array([10, 1, 3, 1, 0, 0])]\n",
        "testData = sc.parallelize(testCandidates)\n",
        "\n",
        "# Train our DecisionTree classifier using our data set\n",
        "model = DecisionTree.trainClassifier(trainingData, numClasses=2,\n",
        "                                     categoricalFeaturesInfo={1:2, 3:4, 4:2, 5:2},\n",
        "                                     impurity='gini', maxDepth=5, maxBins=32)\n",
        "\n",
        "# Now get predictions for our unknown candidates. (Note, you could separate\n",
        "# the source data into a training set and a test set while tuning\n",
        "# parameters and measure accuracy as you go!)\n",
        "predictions = model.predict(testData)\n",
        "print('Hire prediction:')\n",
        "results = predictions.collect()\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "# We can also print out the decision tree itself:\n",
        "print('Learned classification tree model:')\n",
        "print(model.toDebugString())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OQzT9OMf0KR",
        "outputId": "194650a5-d6f4-450b-fcf9-1513d23d4c4f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hire prediction:\n",
            "1.0\n",
            "Learned classification tree model:\n",
            "DecisionTreeModel classifier of depth 4 with 9 nodes\n",
            "  If (feature 1 in {0.0})\n",
            "   If (feature 5 in {0.0})\n",
            "    If (feature 0 <= 0.5)\n",
            "     If (feature 3 in {1.0})\n",
            "      Predict: 0.0\n",
            "     Else (feature 3 not in {1.0})\n",
            "      Predict: 1.0\n",
            "    Else (feature 0 > 0.5)\n",
            "     Predict: 0.0\n",
            "   Else (feature 5 not in {0.0})\n",
            "    Predict: 1.0\n",
            "  Else (feature 1 not in {0.0})\n",
            "   Predict: 1.0\n",
            "\n"
          ]
        }
      ]
    }
  ]
}